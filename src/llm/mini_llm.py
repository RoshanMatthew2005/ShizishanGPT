"""
Mini LLM for agricultural text generation
Placeholder for Phase 2 development
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import yaml


class MiniLLM:
    """
    Mini Language Model for agricultural domain
    Uses pre-trained models with optional fine-tuning
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize Mini LLM
        
        Args:
            config_path: Path to configuration file
        """
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.llm_config = self.config['llm']
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ü§ñ Loading LLM: {self.llm_config['model_name']}")
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_config['model_name'])
        self.model = AutoModelForCausalLM.from_pretrained(
            self.llm_config['model_name']
        ).to(self.device)
        
        print(f"‚úÖ LLM loaded on {self.device}")
    
    def generate(self, prompt: str, max_length: int = None) -> str:
        """
        Generate text from prompt
        
        Args:
            prompt: Input prompt
            max_length: Maximum generation length
            
        Returns:
            Generated text
        """
        if max_length is None:
            max_length = self.llm_config['max_length']
        
        # Encode prompt
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=max_length,
                temperature=self.llm_config['temperature'],
                top_p=self.llm_config['top_p'],
                top_k=self.llm_config['top_k'],
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return generated_text
    
    def format_with_context(self, query: str, context: str) -> str:
        """
        Format query with retrieved context
        
        Args:
            query: User query
            context: Retrieved context from RAG
            
        Returns:
            Formatted prompt
        """
        prompt = f"""You are an agricultural AI assistant.

Context: {context}

Question: {query}

Answer:"""
        
        return prompt


def main():
    """Example usage of MiniLLM"""
    print("ü§ñ Mini LLM Module")
    print("‚ö†Ô∏è Phase 2: To be implemented")
    print("   - Fine-tuning on agricultural data")
    print("   - Integration with RAG retriever")
    print("   - Response generation")


if __name__ == "__main__":
    main()
