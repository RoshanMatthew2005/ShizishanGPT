{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 5622,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02668801708033093,
      "grad_norm": 4.119531154632568,
      "learning_rate": 2.45e-05,
      "loss": 4.6289,
      "step": 50
    },
    {
      "epoch": 0.05337603416066186,
      "grad_norm": 4.708944797515869,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 4.3883,
      "step": 100
    },
    {
      "epoch": 0.08006405124099279,
      "grad_norm": 4.794893264770508,
      "learning_rate": 4.9556320173850056e-05,
      "loss": 4.1927,
      "step": 150
    },
    {
      "epoch": 0.10675206832132372,
      "grad_norm": 5.443695545196533,
      "learning_rate": 4.910358565737052e-05,
      "loss": 4.1326,
      "step": 200
    },
    {
      "epoch": 0.13344008540165467,
      "grad_norm": 4.379207611083984,
      "learning_rate": 4.8650851140890986e-05,
      "loss": 4.0312,
      "step": 250
    },
    {
      "epoch": 0.16012810248198558,
      "grad_norm": 4.546511650085449,
      "learning_rate": 4.819811662441145e-05,
      "loss": 3.9289,
      "step": 300
    },
    {
      "epoch": 0.18681611956231653,
      "grad_norm": 4.115556716918945,
      "learning_rate": 4.7745382107931916e-05,
      "loss": 3.8873,
      "step": 350
    },
    {
      "epoch": 0.21350413664264745,
      "grad_norm": 4.033369064331055,
      "learning_rate": 4.7292647591452374e-05,
      "loss": 3.8467,
      "step": 400
    },
    {
      "epoch": 0.2401921537229784,
      "grad_norm": 4.226182460784912,
      "learning_rate": 4.683991307497284e-05,
      "loss": 3.8309,
      "step": 450
    },
    {
      "epoch": 0.26688017080330934,
      "grad_norm": 3.9827845096588135,
      "learning_rate": 4.6387178558493297e-05,
      "loss": 3.8315,
      "step": 500
    },
    {
      "epoch": 0.29356818788364025,
      "grad_norm": 4.189168453216553,
      "learning_rate": 4.593444404201376e-05,
      "loss": 3.7621,
      "step": 550
    },
    {
      "epoch": 0.32025620496397117,
      "grad_norm": 3.7893624305725098,
      "learning_rate": 4.5481709525534226e-05,
      "loss": 3.7015,
      "step": 600
    },
    {
      "epoch": 0.3469442220443021,
      "grad_norm": 3.4276206493377686,
      "learning_rate": 4.502897500905469e-05,
      "loss": 3.7662,
      "step": 650
    },
    {
      "epoch": 0.37363223912463306,
      "grad_norm": 3.582892656326294,
      "learning_rate": 4.4576240492575156e-05,
      "loss": 3.7065,
      "step": 700
    },
    {
      "epoch": 0.400320256204964,
      "grad_norm": 3.650465488433838,
      "learning_rate": 4.412350597609562e-05,
      "loss": 3.6663,
      "step": 750
    },
    {
      "epoch": 0.4270082732852949,
      "grad_norm": 3.386340856552124,
      "learning_rate": 4.3670771459616085e-05,
      "loss": 3.6378,
      "step": 800
    },
    {
      "epoch": 0.4536962903656258,
      "grad_norm": 3.934461832046509,
      "learning_rate": 4.321803694313655e-05,
      "loss": 3.5691,
      "step": 850
    },
    {
      "epoch": 0.4803843074459568,
      "grad_norm": 3.7237796783447266,
      "learning_rate": 4.276530242665701e-05,
      "loss": 3.5704,
      "step": 900
    },
    {
      "epoch": 0.5070723245262877,
      "grad_norm": 4.471920013427734,
      "learning_rate": 4.231256791017747e-05,
      "loss": 3.6273,
      "step": 950
    },
    {
      "epoch": 0.5337603416066187,
      "grad_norm": 3.7264857292175293,
      "learning_rate": 4.185983339369794e-05,
      "loss": 3.6425,
      "step": 1000
    },
    {
      "epoch": 0.5604483586869495,
      "grad_norm": 4.093287944793701,
      "learning_rate": 4.14070988772184e-05,
      "loss": 3.6428,
      "step": 1050
    },
    {
      "epoch": 0.5871363757672805,
      "grad_norm": 3.2038159370422363,
      "learning_rate": 4.095436436073887e-05,
      "loss": 3.5677,
      "step": 1100
    },
    {
      "epoch": 0.6138243928476115,
      "grad_norm": 3.6470627784729004,
      "learning_rate": 4.0501629844259326e-05,
      "loss": 3.5509,
      "step": 1150
    },
    {
      "epoch": 0.6405124099279423,
      "grad_norm": 3.400747060775757,
      "learning_rate": 4.004889532777979e-05,
      "loss": 3.6241,
      "step": 1200
    },
    {
      "epoch": 0.6672004270082733,
      "grad_norm": 3.0332858562469482,
      "learning_rate": 3.9596160811300255e-05,
      "loss": 3.5318,
      "step": 1250
    },
    {
      "epoch": 0.6938884440886042,
      "grad_norm": 4.063458442687988,
      "learning_rate": 3.914342629482072e-05,
      "loss": 3.5276,
      "step": 1300
    },
    {
      "epoch": 0.7205764611689351,
      "grad_norm": 3.1598992347717285,
      "learning_rate": 3.8690691778341185e-05,
      "loss": 3.5548,
      "step": 1350
    },
    {
      "epoch": 0.7472644782492661,
      "grad_norm": 3.3334829807281494,
      "learning_rate": 3.823795726186164e-05,
      "loss": 3.4773,
      "step": 1400
    },
    {
      "epoch": 0.773952495329597,
      "grad_norm": 3.443749189376831,
      "learning_rate": 3.778522274538211e-05,
      "loss": 3.3526,
      "step": 1450
    },
    {
      "epoch": 0.800640512409928,
      "grad_norm": 3.53852915763855,
      "learning_rate": 3.733248822890257e-05,
      "loss": 3.5314,
      "step": 1500
    },
    {
      "epoch": 0.8273285294902589,
      "grad_norm": 3.3628885746002197,
      "learning_rate": 3.687975371242304e-05,
      "loss": 3.5271,
      "step": 1550
    },
    {
      "epoch": 0.8540165465705898,
      "grad_norm": 2.7735848426818848,
      "learning_rate": 3.64270191959435e-05,
      "loss": 3.4623,
      "step": 1600
    },
    {
      "epoch": 0.8807045636509208,
      "grad_norm": 2.717750310897827,
      "learning_rate": 3.597428467946397e-05,
      "loss": 3.4342,
      "step": 1650
    },
    {
      "epoch": 0.9073925807312516,
      "grad_norm": 3.1038477420806885,
      "learning_rate": 3.552155016298443e-05,
      "loss": 3.5923,
      "step": 1700
    },
    {
      "epoch": 0.9340805978115826,
      "grad_norm": 3.2046282291412354,
      "learning_rate": 3.50688156465049e-05,
      "loss": 3.5003,
      "step": 1750
    },
    {
      "epoch": 0.9607686148919136,
      "grad_norm": 3.396479845046997,
      "learning_rate": 3.4616081130025355e-05,
      "loss": 3.4412,
      "step": 1800
    },
    {
      "epoch": 0.9874566319722444,
      "grad_norm": 3.2104239463806152,
      "learning_rate": 3.416334661354581e-05,
      "loss": 3.4053,
      "step": 1850
    },
    {
      "epoch": 1.0138777688817722,
      "grad_norm": 2.310718059539795,
      "learning_rate": 3.371061209706628e-05,
      "loss": 3.4032,
      "step": 1900
    },
    {
      "epoch": 1.040565785962103,
      "grad_norm": 3.2382848262786865,
      "learning_rate": 3.325787758058674e-05,
      "loss": 3.2827,
      "step": 1950
    },
    {
      "epoch": 1.067253803042434,
      "grad_norm": 2.562941789627075,
      "learning_rate": 3.280514306410721e-05,
      "loss": 3.2814,
      "step": 2000
    },
    {
      "epoch": 1.0939418201227649,
      "grad_norm": 3.5534679889678955,
      "learning_rate": 3.235240854762767e-05,
      "loss": 3.3505,
      "step": 2050
    },
    {
      "epoch": 1.1206298372030958,
      "grad_norm": 3.617924928665161,
      "learning_rate": 3.189967403114814e-05,
      "loss": 3.2703,
      "step": 2100
    },
    {
      "epoch": 1.1473178542834268,
      "grad_norm": 2.7729854583740234,
      "learning_rate": 3.14469395146686e-05,
      "loss": 3.2988,
      "step": 2150
    },
    {
      "epoch": 1.1740058713637578,
      "grad_norm": 2.6969099044799805,
      "learning_rate": 3.0994204998189066e-05,
      "loss": 3.3693,
      "step": 2200
    },
    {
      "epoch": 1.2006938884440885,
      "grad_norm": 2.7736573219299316,
      "learning_rate": 3.0541470481709524e-05,
      "loss": 3.2783,
      "step": 2250
    },
    {
      "epoch": 1.2273819055244195,
      "grad_norm": 2.7536845207214355,
      "learning_rate": 3.0088735965229993e-05,
      "loss": 3.3226,
      "step": 2300
    },
    {
      "epoch": 1.2540699226047505,
      "grad_norm": 2.9595394134521484,
      "learning_rate": 2.9636001448750454e-05,
      "loss": 3.2558,
      "step": 2350
    },
    {
      "epoch": 1.2807579396850814,
      "grad_norm": 2.7887978553771973,
      "learning_rate": 2.918326693227092e-05,
      "loss": 3.2637,
      "step": 2400
    },
    {
      "epoch": 1.3074459567654122,
      "grad_norm": 3.4746344089508057,
      "learning_rate": 2.8730532415791384e-05,
      "loss": 3.3326,
      "step": 2450
    },
    {
      "epoch": 1.3341339738457432,
      "grad_norm": 3.0288350582122803,
      "learning_rate": 2.827779789931185e-05,
      "loss": 3.2694,
      "step": 2500
    },
    {
      "epoch": 1.3608219909260741,
      "grad_norm": 2.799049139022827,
      "learning_rate": 2.7825063382832307e-05,
      "loss": 3.2941,
      "step": 2550
    },
    {
      "epoch": 1.3875100080064051,
      "grad_norm": 3.369053840637207,
      "learning_rate": 2.737232886635277e-05,
      "loss": 3.2969,
      "step": 2600
    },
    {
      "epoch": 1.414198025086736,
      "grad_norm": 2.854861259460449,
      "learning_rate": 2.6919594349873233e-05,
      "loss": 3.2872,
      "step": 2650
    },
    {
      "epoch": 1.440886042167067,
      "grad_norm": 3.180312395095825,
      "learning_rate": 2.6466859833393698e-05,
      "loss": 3.3312,
      "step": 2700
    },
    {
      "epoch": 1.467574059247398,
      "grad_norm": 2.8675994873046875,
      "learning_rate": 2.6014125316914162e-05,
      "loss": 3.312,
      "step": 2750
    },
    {
      "epoch": 1.4942620763277288,
      "grad_norm": 3.2700438499450684,
      "learning_rate": 2.5561390800434627e-05,
      "loss": 3.2688,
      "step": 2800
    },
    {
      "epoch": 1.5209500934080598,
      "grad_norm": 3.1603212356567383,
      "learning_rate": 2.510865628395509e-05,
      "loss": 3.2405,
      "step": 2850
    },
    {
      "epoch": 1.5476381104883907,
      "grad_norm": 3.0227224826812744,
      "learning_rate": 2.4655921767475554e-05,
      "loss": 3.3246,
      "step": 2900
    },
    {
      "epoch": 1.5743261275687215,
      "grad_norm": 3.449763774871826,
      "learning_rate": 2.420318725099602e-05,
      "loss": 3.2003,
      "step": 2950
    },
    {
      "epoch": 1.6010141446490525,
      "grad_norm": 3.611067056655884,
      "learning_rate": 2.3750452734516483e-05,
      "loss": 3.2018,
      "step": 3000
    },
    {
      "epoch": 1.6277021617293834,
      "grad_norm": 3.5873186588287354,
      "learning_rate": 2.3297718218036945e-05,
      "loss": 3.2447,
      "step": 3050
    },
    {
      "epoch": 1.6543901788097144,
      "grad_norm": 3.4448952674865723,
      "learning_rate": 2.2844983701557406e-05,
      "loss": 3.2529,
      "step": 3100
    },
    {
      "epoch": 1.6810781958900454,
      "grad_norm": 2.7467024326324463,
      "learning_rate": 2.239224918507787e-05,
      "loss": 3.3081,
      "step": 3150
    },
    {
      "epoch": 1.7077662129703763,
      "grad_norm": 2.795412540435791,
      "learning_rate": 2.1939514668598336e-05,
      "loss": 3.3025,
      "step": 3200
    },
    {
      "epoch": 1.7344542300507073,
      "grad_norm": 3.4534740447998047,
      "learning_rate": 2.14867801521188e-05,
      "loss": 3.2227,
      "step": 3250
    },
    {
      "epoch": 1.7611422471310383,
      "grad_norm": 2.5699055194854736,
      "learning_rate": 2.1034045635639262e-05,
      "loss": 3.2329,
      "step": 3300
    },
    {
      "epoch": 1.7878302642113693,
      "grad_norm": 3.2343599796295166,
      "learning_rate": 2.0581311119159727e-05,
      "loss": 3.2727,
      "step": 3350
    },
    {
      "epoch": 1.8145182812917,
      "grad_norm": 2.75089430809021,
      "learning_rate": 2.0128576602680188e-05,
      "loss": 3.3074,
      "step": 3400
    },
    {
      "epoch": 1.841206298372031,
      "grad_norm": 3.038896083831787,
      "learning_rate": 1.9675842086200653e-05,
      "loss": 3.3068,
      "step": 3450
    },
    {
      "epoch": 1.8678943154523617,
      "grad_norm": 2.8521063327789307,
      "learning_rate": 1.9223107569721114e-05,
      "loss": 3.1605,
      "step": 3500
    },
    {
      "epoch": 1.8945823325326927,
      "grad_norm": 2.9447410106658936,
      "learning_rate": 1.877037305324158e-05,
      "loss": 3.2288,
      "step": 3550
    },
    {
      "epoch": 1.9212703496130237,
      "grad_norm": 3.417564630508423,
      "learning_rate": 1.8317638536762044e-05,
      "loss": 3.3042,
      "step": 3600
    },
    {
      "epoch": 1.9479583666933546,
      "grad_norm": 3.0282208919525146,
      "learning_rate": 1.786490402028251e-05,
      "loss": 3.1444,
      "step": 3650
    },
    {
      "epoch": 1.9746463837736856,
      "grad_norm": 3.5233473777770996,
      "learning_rate": 1.7412169503802974e-05,
      "loss": 3.2156,
      "step": 3700
    },
    {
      "epoch": 2.0010675206832134,
      "grad_norm": 2.7674169540405273,
      "learning_rate": 1.695943498732343e-05,
      "loss": 3.2045,
      "step": 3750
    },
    {
      "epoch": 2.0277555377635443,
      "grad_norm": 3.3525896072387695,
      "learning_rate": 1.6506700470843896e-05,
      "loss": 3.1048,
      "step": 3800
    },
    {
      "epoch": 2.0544435548438753,
      "grad_norm": 2.387676239013672,
      "learning_rate": 1.605396595436436e-05,
      "loss": 3.2069,
      "step": 3850
    },
    {
      "epoch": 2.081131571924206,
      "grad_norm": 3.504976749420166,
      "learning_rate": 1.5601231437884826e-05,
      "loss": 3.1003,
      "step": 3900
    },
    {
      "epoch": 2.107819589004537,
      "grad_norm": 3.237985849380493,
      "learning_rate": 1.514849692140529e-05,
      "loss": 3.1905,
      "step": 3950
    },
    {
      "epoch": 2.134507606084868,
      "grad_norm": 2.828970193862915,
      "learning_rate": 1.4695762404925752e-05,
      "loss": 3.1575,
      "step": 4000
    },
    {
      "epoch": 2.1611956231651988,
      "grad_norm": 2.51627516746521,
      "learning_rate": 1.4243027888446217e-05,
      "loss": 3.0891,
      "step": 4050
    },
    {
      "epoch": 2.1878836402455297,
      "grad_norm": 2.6515586376190186,
      "learning_rate": 1.3790293371966679e-05,
      "loss": 3.2264,
      "step": 4100
    },
    {
      "epoch": 2.2145716573258607,
      "grad_norm": 3.0680184364318848,
      "learning_rate": 1.3337558855487142e-05,
      "loss": 3.1936,
      "step": 4150
    },
    {
      "epoch": 2.2412596744061917,
      "grad_norm": 2.9310901165008545,
      "learning_rate": 1.2884824339007607e-05,
      "loss": 3.2523,
      "step": 4200
    },
    {
      "epoch": 2.2679476914865226,
      "grad_norm": 3.1540119647979736,
      "learning_rate": 1.243208982252807e-05,
      "loss": 3.1935,
      "step": 4250
    },
    {
      "epoch": 2.2946357085668536,
      "grad_norm": 3.053964853286743,
      "learning_rate": 1.1979355306048534e-05,
      "loss": 3.15,
      "step": 4300
    },
    {
      "epoch": 2.3213237256471846,
      "grad_norm": 2.7415578365325928,
      "learning_rate": 1.1526620789568998e-05,
      "loss": 3.257,
      "step": 4350
    },
    {
      "epoch": 2.3480117427275156,
      "grad_norm": 2.896530866622925,
      "learning_rate": 1.107388627308946e-05,
      "loss": 3.1478,
      "step": 4400
    },
    {
      "epoch": 2.374699759807846,
      "grad_norm": 3.648237466812134,
      "learning_rate": 1.0621151756609926e-05,
      "loss": 3.135,
      "step": 4450
    },
    {
      "epoch": 2.401387776888177,
      "grad_norm": 3.3596174716949463,
      "learning_rate": 1.0168417240130389e-05,
      "loss": 3.148,
      "step": 4500
    },
    {
      "epoch": 2.428075793968508,
      "grad_norm": 3.154567241668701,
      "learning_rate": 9.715682723650852e-06,
      "loss": 3.1222,
      "step": 4550
    },
    {
      "epoch": 2.454763811048839,
      "grad_norm": 2.791860580444336,
      "learning_rate": 9.262948207171315e-06,
      "loss": 3.1998,
      "step": 4600
    },
    {
      "epoch": 2.48145182812917,
      "grad_norm": 2.7441043853759766,
      "learning_rate": 8.81021369069178e-06,
      "loss": 3.1482,
      "step": 4650
    },
    {
      "epoch": 2.508139845209501,
      "grad_norm": 3.2708687782287598,
      "learning_rate": 8.357479174212241e-06,
      "loss": 3.1711,
      "step": 4700
    },
    {
      "epoch": 2.534827862289832,
      "grad_norm": 3.135343551635742,
      "learning_rate": 7.904744657732706e-06,
      "loss": 3.151,
      "step": 4750
    },
    {
      "epoch": 2.561515879370163,
      "grad_norm": 2.7381319999694824,
      "learning_rate": 7.45201014125317e-06,
      "loss": 3.2032,
      "step": 4800
    },
    {
      "epoch": 2.588203896450494,
      "grad_norm": 3.2109487056732178,
      "learning_rate": 6.999275624773632e-06,
      "loss": 3.1056,
      "step": 4850
    },
    {
      "epoch": 2.6148919135308244,
      "grad_norm": 3.409000873565674,
      "learning_rate": 6.546541108294096e-06,
      "loss": 3.1803,
      "step": 4900
    },
    {
      "epoch": 2.6415799306111554,
      "grad_norm": 2.4994046688079834,
      "learning_rate": 6.09380659181456e-06,
      "loss": 3.1141,
      "step": 4950
    },
    {
      "epoch": 2.6682679476914863,
      "grad_norm": 3.6410558223724365,
      "learning_rate": 5.641072075335023e-06,
      "loss": 3.0767,
      "step": 5000
    },
    {
      "epoch": 2.6949559647718173,
      "grad_norm": 2.959196090698242,
      "learning_rate": 5.188337558855488e-06,
      "loss": 3.033,
      "step": 5050
    },
    {
      "epoch": 2.7216439818521483,
      "grad_norm": 2.930100440979004,
      "learning_rate": 4.735603042375951e-06,
      "loss": 3.1333,
      "step": 5100
    },
    {
      "epoch": 2.7483319989324793,
      "grad_norm": 2.900897264480591,
      "learning_rate": 4.282868525896414e-06,
      "loss": 3.012,
      "step": 5150
    },
    {
      "epoch": 2.7750200160128102,
      "grad_norm": 3.0017900466918945,
      "learning_rate": 3.830134009416878e-06,
      "loss": 3.2183,
      "step": 5200
    },
    {
      "epoch": 2.801708033093141,
      "grad_norm": 3.3072359561920166,
      "learning_rate": 3.3773994929373414e-06,
      "loss": 3.1737,
      "step": 5250
    },
    {
      "epoch": 2.828396050173472,
      "grad_norm": 2.703619956970215,
      "learning_rate": 2.924664976457805e-06,
      "loss": 3.1337,
      "step": 5300
    },
    {
      "epoch": 2.855084067253803,
      "grad_norm": 2.8011460304260254,
      "learning_rate": 2.471930459978269e-06,
      "loss": 3.1472,
      "step": 5350
    },
    {
      "epoch": 2.881772084334134,
      "grad_norm": 2.719444751739502,
      "learning_rate": 2.0191959434987324e-06,
      "loss": 3.15,
      "step": 5400
    },
    {
      "epoch": 2.908460101414465,
      "grad_norm": 2.639557123184204,
      "learning_rate": 1.566461427019196e-06,
      "loss": 3.1543,
      "step": 5450
    },
    {
      "epoch": 2.935148118494796,
      "grad_norm": 2.8465332984924316,
      "learning_rate": 1.1137269105396597e-06,
      "loss": 3.1804,
      "step": 5500
    },
    {
      "epoch": 2.961836135575127,
      "grad_norm": 2.9538803100585938,
      "learning_rate": 6.609923940601232e-07,
      "loss": 3.1642,
      "step": 5550
    },
    {
      "epoch": 2.9885241526554576,
      "grad_norm": 2.9655089378356934,
      "learning_rate": 2.0825787758058673e-07,
      "loss": 3.1325,
      "step": 5600
    }
  ],
  "logging_steps": 50,
  "max_steps": 5622,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4773918171267072.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
